{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import mindspore\n",
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops.composite as C\n",
    "import mindspore.ops.functional as F\n",
    "import mindspore.ops.operations as P\n",
    "from mindspore import Tensor, Parameter\n",
    "from rnn import RNN, WithLossCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-advocacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data, num_dic, n_step):\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n",
    "\n",
    "        input = [num_dic[n] for n in seq[0]]\n",
    "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        output_batch.append(np.eye(n_class)[output])\n",
    "        target_batch.append(target) # not one-hot\n",
    "\n",
    "    # make tensor\n",
    "    return Tensor(input_batch, mindspore.float32), Tensor(output_batch, mindspore.float32), Tensor(target_batch, mindspore.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Seq2Seq(nn.Cell):\n",
    "    def __init__(self, n_class, n_hidden, dropout):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.enc_cell = RNN(input_size=n_class, hidden_size=n_hidden, dropout=dropout)\n",
    "        self.dec_cell = RNN(input_size=n_class, hidden_size=n_hidden, dropout=dropout)\n",
    "        self.fc = nn.Dense(n_hidden, n_class)\n",
    "        \n",
    "        self.transpose = P.Transpose()\n",
    "        \n",
    "    def construct(self, enc_input, enc_hidden, dec_input):\n",
    "        enc_input = self.transpose(enc_input, (1, 0, 2)) # enc_input: [max_len(=n_step, time step), batch_size, n_class]\n",
    "        dec_input = self.transpose(dec_input, (1, 0, 2)) # dec_input: [max_len(=n_step, time step), batch_size, n_class]\n",
    "\n",
    "        # enc_states : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        _, enc_states = self.enc_cell(enc_input, enc_hidden)\n",
    "        # outputs : [max_len+1(=6), batch_size, num_directions(=1) * n_hidden(=128)]\n",
    "        outputs, _ = self.dec_cell(dec_input, enc_states)\n",
    "\n",
    "        model = self.fc(outputs) # model : [max_len+1(=6), batch_size, n_class]\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 5\n",
    "n_hidden = 128\n",
    "dropout = 0.5\n",
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "\n",
    "n_class = len(num_dic)\n",
    "batch_size = len(seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(n_class, n_hidden, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "optimizer = nn.Adam(model.get_parameters(), learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, output_batch, target_batch = make_batch(seq_data, num_dic, n_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import context\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_with_criterion = WithLossCell(model, criterion)\n",
    "train_network = nn.TrainOneStepCell(net_with_criterion, optimizer)\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
    "    hidden = Tensor(np.zeros((batch_size, n_hidden)), mindspore.float32)\n",
    "    # input_batch : [batch_size, max_len(=n_step, time step), n_class]\n",
    "    # output_batch : [batch_size, max_len+1(=n_step, time step) (becase of 'S' or 'E'), n_class]\n",
    "    # target_batch : [batch_size, max_len+1(=n_step, time step)], not one-hot\n",
    "    loss = train_network(input_batch, hidden, output_batch, target_batch)\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss.asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "def translate(word):\n",
    "    input_batch, output_batch, _ = make_batch([[word, 'P' * len(word)]],  num_dic, n_step)\n",
    "    # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
    "    hidden = Tensor(np.zeros((1, n_hidden)), mindspore.float32)\n",
    "    output = model(input_batch, hidden, output_batch)\n",
    "    # output : [max_len+1(=6), batch_size(=1), n_class]\n",
    "\n",
    "    predict = output.asnumpy().argmax(2) # select n_class dimension\n",
    "    decoded = [char_arr[i[0]] for i in predict]\n",
    "    end = decoded.index('E')\n",
    "    translated = ''.join(decoded[:end])\n",
    "\n",
    "    return translated.replace('P', '')\n",
    "\n",
    "print('test')\n",
    "print('man ->', translate('man'))\n",
    "print('mans ->', translate('mans'))\n",
    "print('king ->', translate('king'))\n",
    "print('black ->', translate('black'))\n",
    "print('upp ->', translate('upp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-deposit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
